{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d257e1-bfaf-4b43-a6f2-434e2dc90d3a",
   "metadata": {},
   "source": [
    "# 1. Importar CSVs e salvar como JSON na pasta municipios-estados/json/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1de80c-da58-4249-8474-6361aa05be01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configuração do Spark\n",
    "spark = SparkSession.builder.appName(\"CSV_to_JSON\").getOrCreate()\n",
    "\n",
    "# Caminhos de entrada e saída\n",
    "input_path = \"municipios-estados/csv/\"\n",
    "output_path = \"municipios-estados/json/\"\n",
    "\n",
    "# Criar a pasta de saída, se não existir\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Listar todos os arquivos CSV na pasta\n",
    "csv_files = [os.path.join(input_path, file) for file in os.listdir(input_path) if file.endswith('.csv')]\n",
    "\n",
    "# Converter cada CSV para JSON\n",
    "for csv_file in csv_files:\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    json_file_name = os.path.basename(csv_file).replace(\".csv\", \".json\")\n",
    "    df.write.mode(\"overwrite\").json(os.path.join(output_path, json_file_name))\n",
    "\n",
    "print(f\"Todos os arquivos CSV foram convertidos para JSON em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7c9b6-4962-4485-b309-92ffb5d1c266",
   "metadata": {},
   "source": [
    "# 2. Importar JSONs para o MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5cc78b-4790-4ef1-8dcb-ead4e73504d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuração do MinIO\n",
    "MINIO_ENDPOINT = os.environ.get('MINIO_ENDPOINT')\n",
    "MINIO_ACCESS_KEY = os.environ.get('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY = os.environ.get('MINIO_SECRET_KEY')\n",
    "BUCKET_NAME = \"aula-08\"\n",
    "\n",
    "# Caminho da pasta com os JSONs\n",
    "json_folder = \"municipios-estados/json/\"\n",
    "\n",
    "# Conexão com o MinIO\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY,\n",
    ")\n",
    "\n",
    "# Certificar-se de que o bucket existe\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=BUCKET_NAME)\n",
    "except Exception:\n",
    "    s3_client.create_bucket(Bucket=BUCKET_NAME)\n",
    "    print(f\"Bucket '{BUCKET_NAME}' criado com sucesso!\")\n",
    "\n",
    "# Enviar os arquivos JSON para o MinIO\n",
    "for root, _, files in os.walk(json_folder):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        object_name = os.path.relpath(file_path, json_folder)\n",
    "        s3_client.upload_file(file_path, BUCKET_NAME, object_name)\n",
    "        print(f\"Arquivo '{file_name}' enviado com sucesso para o bucket '{BUCKET_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f16198-e646-4141-a1a8-38b2d9ea115d",
   "metadata": {},
   "source": [
    "# 3. Importar CSVs e salvar como arquivo .parquet no MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330244d-f798-4794-93fe-23592ab70301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caminho de entrada e saída\n",
    "input_path = \"municipios-estados/csv/\"\n",
    "parquet_output_path = \"s3a://aula-08/municipios-parquet/\"\n",
    "\n",
    "# Configuração do Spark para MinIO\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", os.environ.get('MINIO_SPARK'))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.environ.get('MINIO_ACCESS_KEY'))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.environ.get('MINIO_SECRET_KEY'))\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.endpoint.region\", 'sa-east-1')\n",
    "\n",
    "# Converter CSVs para Parquet\n",
    "for csv_file in csv_files:\n",
    "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    parquet_file_name = os.path.basename(csv_file).replace(\".csv\", \".parquet\")\n",
    "    df.write.mode(\"overwrite\").parquet(os.path.join(parquet_output_path, parquet_file_name))\n",
    "\n",
    "print(f\"Todos os arquivos CSV foram convertidos para Parquet e salvos no MinIO.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
